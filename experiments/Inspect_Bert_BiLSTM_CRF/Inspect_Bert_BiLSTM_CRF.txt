21-11-04 18:08:31, DEBUG: Attempting to acquire lock 140638913239632 on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock
21-11-04 18:08:31, DEBUG: Lock 140638913239632 acquired on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock
21-11-04 18:08:31, DEBUG: Attempting to release lock 140638913239632 on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock
21-11-04 18:08:31, DEBUG: Lock 140638913239632 released on /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock
21-11-04 18:08:32, DEBUG: Attempting to acquire lock 140638904894672 on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock
21-11-04 18:08:32, DEBUG: Lock 140638904894672 acquired on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock
21-11-04 18:08:49, DEBUG: Attempting to release lock 140638904894672 on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock
21-11-04 18:08:49, DEBUG: Lock 140638904894672 released on /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock
21-11-04 18:09:00, INFO: 
***************** INSPECT_BERT_BILSTM_CRF **************
21-11-04 18:09:00, INFO: batch_size: 4
21-11-04 18:09:00, INFO: data_name: Inspec
21-11-04 18:09:00, INFO: dropout: 0.2
21-11-04 18:09:00, INFO: file_name: Inspect_Bert_BiLSTM_CRF
21-11-04 18:09:00, INFO: hidden_size: 128
21-11-04 18:09:00, INFO: lr: 0.0001
21-11-04 18:09:00, INFO: num_epoch: 10
21-11-04 18:09:00, INFO: num_epoch_save: 5.0
21-11-04 18:09:00, INFO: num_layers: 1
21-11-04 18:09:00, INFO: test_size: 0.2
21-11-04 18:09:00, INFO: --------------------------------
21-11-04 18:09:00, INFO: BERT_BiLSTM_CRF(
  (lstm): LSTM(768, 64, batch_first=True, bidirectional=True)
  (dropout): Dropout(p=0.2, inplace=False)
  (bert): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (hidden2tag): Linear(in_features=128, out_features=3, bias=True)
  (crf): CRF(num_tags=3)
)
21-11-04 18:09:00, INFO: --------------------------------
21-11-04 18:14:31, INFO: ---------------------------------------Epoch 1---------------------------------------
21-11-04 18:14:31, INFO: Train: loss 20.768367410004885, precision_score 0.4717050001566316, recall_score 0.340668598645064, f1_score 0.3207556009697408
21-11-04 18:14:31, INFO: Test: loss 15.533876757621766, precision_score 0.4750157444099236, recall_score 0.38931575382715217, f1_score 0.396678395553832
21-11-04 18:19:57, INFO: ---------------------------------------Epoch 2---------------------------------------
21-11-04 18:19:57, INFO: Train: loss 13.797418698332363, precision_score 0.6977898165400737, recall_score 0.44359366220326485, f1_score 0.4638054123355735
21-11-04 18:19:57, INFO: Test: loss 12.31050136089325, precision_score 0.696841113667607, recall_score 0.5130950205877495, f1_score 0.5492794244894429
21-11-04 18:25:22, INFO: ---------------------------------------Epoch 3---------------------------------------
21-11-04 18:25:22, INFO: Train: loss 11.549033883819007, precision_score 0.7068309189534241, recall_score 0.5569632198827971, f1_score 0.5998799362544602
21-11-04 18:25:22, INFO: Test: loss 10.947788114547729, precision_score 0.7207014804540935, recall_score 0.5430572033001696, f1_score 0.5955364759449234
21-11-04 18:30:49, INFO: ---------------------------------------Epoch 4---------------------------------------
21-11-04 18:30:49, INFO: Train: loss 10.445279606004108, precision_score 0.7224123563601185, recall_score 0.6156937514246099, f1_score 0.6547501411060492
21-11-04 18:30:49, INFO: Test: loss 10.159123136997223, precision_score 0.7132978164131655, recall_score 0.6480398819285861, f1_score 0.6743550586635764
21-11-04 18:36:18, INFO: ---------------------------------------Epoch 5---------------------------------------
21-11-04 18:36:18, INFO: Train: loss 9.759582622308182, precision_score 0.7334856720297535, recall_score 0.6531210543671395, f1_score 0.6855117643956395
21-11-04 18:36:18, INFO: Test: loss 9.649091453552247, precision_score 0.7309857968756566, recall_score 0.6426224852304073, f1_score 0.6778164592985698
21-11-04 18:41:57, INFO: ---------------------------------------Epoch 6---------------------------------------
21-11-04 18:41:57, INFO: Train: loss 9.254106699673454, precision_score 0.7432858898516396, recall_score 0.6730222890771423, f1_score 0.7022100548783538
21-11-04 18:41:57, INFO: Test: loss 9.343746271133423, precision_score 0.7427276285007093, recall_score 0.6302287017856114, f1_score 0.6740706683062815
21-11-04 18:47:22, INFO: ---------------------------------------Epoch 7---------------------------------------
21-11-04 18:47:22, INFO: Train: loss 8.843260613300448, precision_score 0.7512227972472729, recall_score 0.6888310002598989, f1_score 0.7152117770199377
21-11-04 18:47:22, INFO: Test: loss 9.0065092086792, precision_score 0.7228964442289861, recall_score 0.7020834941023867, f1_score 0.7101164192847144
21-11-04 18:52:49, INFO: ---------------------------------------Epoch 8---------------------------------------
21-11-04 18:52:49, INFO: Train: loss 8.46481047895618, precision_score 0.7588361649559913, recall_score 0.6995220454321017, f1_score 0.7247927438146741
21-11-04 18:52:49, INFO: Test: loss 8.731706786155701, precision_score 0.7258713509139415, recall_score 0.7038681130777363, f1_score 0.7131256638777229
21-11-04 18:58:12, INFO: ---------------------------------------Epoch 9---------------------------------------
21-11-04 18:58:12, INFO: Train: loss 8.134520525920362, precision_score 0.7642986266336317, recall_score 0.7089945180535754, f1_score 0.7327236392510855
21-11-04 18:58:12, INFO: Test: loss 8.496957802772522, precision_score 0.7300289958755025, recall_score 0.7086993222432767, f1_score 0.7173325910580816
21-11-04 19:03:36, INFO: ---------------------------------------Epoch 10---------------------------------------
21-11-04 19:03:36, INFO: Train: loss 7.822789813641617, precision_score 0.7693548643776816, recall_score 0.7178750881887089, f1_score 0.7401717095358787
21-11-04 19:03:36, INFO: Test: loss 8.28219220161438, precision_score 0.7352696426172532, recall_score 0.6820501631685777, f1_score 0.7050977737089318
21-11-04 19:03:36, INFO: --------------------------------------------------------------------------------------
21-11-04 19:03:36, INFO:                                        Result                                         
21-11-04 19:03:36, INFO: --------------------------------------------------------------------------------------
21-11-04 19:05:13, INFO: Train:
21-11-04 19:05:14, INFO: 
              precision    recall  f1-score   support

           B       0.69      0.52      0.60     57656
           I       0.72      0.69      0.70    113448
           O       0.93      0.95      0.94    849568

    accuracy                           0.90   1020672
   macro avg       0.78      0.72      0.75   1020672
weighted avg       0.89      0.90      0.89   1020672

21-11-04 19:05:14, INFO: Test: 
21-11-04 19:05:14, INFO: 
              precision    recall  f1-score   support

           B       0.64      0.49      0.55      3649
           I       0.65      0.62      0.63      6899
           O       0.92      0.94      0.93     52115

    accuracy                           0.88     62663
   macro avg       0.74      0.68      0.71     62663
weighted avg       0.87      0.88      0.87     62663

